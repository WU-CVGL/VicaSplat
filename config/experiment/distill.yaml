# @package _global_

defaults:
  - /dataset@_group_.re10k: re10k
  - override /model/encoder: vicasplat
  - override /model/encoder/backbone: vica
  - override /loss: [mse, lpips, camera]

wandb:
  name: vicasplat_distill
  tags: [re10k, 256x256]

dataset:
  re10k:
    # overfit_to_scene: 2645536c2312c35a
    view_sampler:
      warm_up_steps: 5_000

model:
  encoder:
    gs_center_head_type: dpt
    gs_param_head_type: dpt_gs
    pretrained_weights: checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth
    predict_conf: true
    backbone:
      use_blocked_causal_attention: true
      use_intrinsic_embedding: true
  decoder:
    use_gsplat: false
    make_scale_invariant: false
  

train:
  distiller: 'mast3r'
  distill_only_steps: 10_000
  distill_max_steps: 10_000
  gradient_checkpointing: false
  extended_visualization: true
  sh_warmup_every_n_steps: -1
  n_camera_opt_views: 0
  distill_weight: 1.0

loss:
  camera:
    weight: 1.0

optimizer:
  lr: 4e-5
  warm_up_steps: 500
  backbone_lr_multiplier: 1.0

data_loader:
  # Avoid having to spin up new processes to print out visualizations.
  train:
    re10k:
      num_workers: 4
      persistent_workers: true
      batch_size: 24
      seed: 1234
  test:
    num_workers: 4
    persistent_workers: false
    batch_size: 1
    seed: 2345
  val:
    re10k:
      num_workers: 1
      persistent_workers: true
      batch_size: 1
      seed: 3456
    dl3dv:
      num_workers: 1
      persistent_workers: true
      batch_size: 1
      seed: 3456
    mvimgnet:
      num_workers: 1
      persistent_workers: true
      batch_size: 1
      seed: 3456

trainer:
  max_steps: 10_000
  val_check_interval: 500
  # precision: bf16-mixed
  gradient_clip_val: 1.0

checkpointing:
  every_n_train_steps: 5_000
  save_weights_only: false
